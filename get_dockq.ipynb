{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for extracting information from the files created by piping the DockQ results to text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for each line of the file\n",
    "\n",
    "def autoinhibitory_pdb_full_depth(line):\n",
    "    # Line looks like: \"Model : ./autoinhibitory/native/{uniprot}_{pdb}.pdb\"\n",
    "\n",
    "    id_dict = {}\n",
    "    fp = line.split()[2]\n",
    "    fn = fp.split('/')[3]\n",
    "    ids = fn.split('_')\n",
    "    uniprot = ids[0]\n",
    "    pdb = ids[1].split('.')[0]\n",
    "\n",
    "    id_dict['uniprot'] = uniprot\n",
    "    id_dict['pdb'] = pdb\n",
    "\n",
    "    return id_dict\n",
    "\n",
    "\n",
    "def pdb_cluster_ids(line):\n",
    "    # Line looks like: \"Model  : /share/scratch/bjechow/dockq/md_pdb_cluster/native/{cluster}_{pdb}.pdb\"\n",
    "\n",
    "    id_dict = {}\n",
    "    fp = line.split()[2]\n",
    "    fn = fp.split('/')[8]\n",
    "    ids = fn.split('_')\n",
    "    cluster = ids[0]\n",
    "    pdb = ids[1].split('.')[0]\n",
    "\n",
    "    id_dict['cluster'] = cluster\n",
    "    id_dict['pdb'] = pdb\n",
    "\n",
    "    return id_dict\n",
    "\n",
    "def full_depth_cluster_ids(line):\n",
    "    # Line looks like: \"Model  : /share/scratch/bjechow/dockq/md_full_depth_cluster/native/{uniprot}_{cluster}.pdb\"\n",
    "\n",
    "    id_dict = {}\n",
    "    fp = line.split()[2]\n",
    "    fn = fp.split('/')[7]\n",
    "    ids = fn.split('_')\n",
    "    uniprot = ids[0]\n",
    "    cluster = ids[1].split('.')[0]\n",
    "\n",
    "    id_dict['uniprot'] = uniprot\n",
    "    id_dict['cluster'] = cluster\n",
    "\n",
    "    return id_dict\n",
    "\n",
    "def pdb_full_depth_ids(line):\n",
    "    # Line looks like: \"Model : /share/scratch/bjechow/dockq/md_pdb_full_depth/native/{uniprot}_{pdb}.pdb\"\n",
    "\n",
    "    id_dict = {}\n",
    "    fp = line.split()[2]\n",
    "    fn = fp.split('/')[7]\n",
    "    ids = fn.split('_')\n",
    "    uniprot = ids[0]\n",
    "    pdb = ids[1].split('.')[0]\n",
    "\n",
    "    id_dict['uniprot'] = uniprot\n",
    "    id_dict['pdb'] = pdb\n",
    "\n",
    "    return id_dict\n",
    "\n",
    "def get_fnat(line):\n",
    "    # Line looks like: \"Fnat 0.804 123 correct of 153 native contacts\"\n",
    "    fnat_dict = {}\n",
    "\n",
    "    fnat = float(line.split()[1])\n",
    "    fnat_correct = int(line.split()[2])\n",
    "    fnat_total = int(line.split()[5])\n",
    "\n",
    "    fnat_dict['fnat'] = fnat\n",
    "    fnat_dict['fnat_correct'] = fnat_correct\n",
    "    fnat_dict['fnat_total'] = fnat_total\n",
    "\n",
    "    return fnat_dict\n",
    "\n",
    "def get_fnonnat(line):\n",
    "    # Line looks like: \"Fnonnat 0.134 19 non-native of 142 model contacts\"\n",
    "    fnonnat_dict = {}\n",
    "\n",
    "    fnonnat = float(line.split()[1])\n",
    "    fnonnat_nnative = int(line.split()[2])\n",
    "    fnonnat_model = int(line.split()[5])\n",
    "\n",
    "    fnonnat_dict['fnonnat'] = fnonnat\n",
    "    fnonnat_dict['fnonnat_nnative'] = fnonnat_nnative\n",
    "    fnonnat_dict['fnonnat_model'] = fnonnat_model\n",
    "\n",
    "    return fnonnat_dict\n",
    "\n",
    "def get_irms(line):\n",
    "    # Line looks like: \"iRMS 1.297\"\n",
    "    irms_dict = {}\n",
    "\n",
    "    irms = float(line.split()[1])\n",
    "\n",
    "    irms_dict['irms'] = irms\n",
    "\n",
    "    return irms_dict\n",
    "\n",
    "def get_lrms(line):\n",
    "    # Line looks like: \"LRMS 1.027\"\n",
    "    lrms_dict = {}\n",
    "\n",
    "    lrms = float(line.split()[1])\n",
    "\n",
    "    lrms_dict['lrms'] = lrms\n",
    "\n",
    "    return lrms_dict\n",
    "\n",
    "def get_dockq(line):\n",
    "    # Line looks like: \"DockQ 0.421\"\n",
    "    dockq_dict = {}\n",
    "\n",
    "    dockq = float(line.split()[1])\n",
    "\n",
    "    dockq_dict['dockq'] = dockq\n",
    "\n",
    "    return dockq_dict\n",
    "\n",
    "def file_info(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    info = {}\n",
    "    for line in lines:\n",
    "        if line.startswith('Model'):\n",
    "            if 'pdb_cluster' in line:\n",
    "                info.update(pdb_cluster_ids(line))\n",
    "            elif 'full_depth_cluster' in line:\n",
    "                info.update(full_depth_cluster_ids(line))\n",
    "            elif 'pdb_full_depth' in line:\n",
    "                info.update(pdb_full_depth_ids(line))\n",
    "            elif 'autoinhibitory' in line:\n",
    "                info.update(autoinhibitory_pdb_full_depth(line))\n",
    "            else:\n",
    "                info.update(pdb_cluster_ids(line))\n",
    "\n",
    "        elif line.startswith('Fnat'):\n",
    "            info.update(get_fnat(line))\n",
    "\n",
    "        elif line.startswith('Fnonnat'):\n",
    "            info.update(get_fnonnat(line))\n",
    "\n",
    "        elif line.startswith('iRMS'):\n",
    "            info.update(get_irms(line))\n",
    "\n",
    "        elif line.startswith('LRMS'):\n",
    "            info.update(get_lrms(line))\n",
    "        \n",
    "        elif line.startswith('DockQ'):\n",
    "            info.update(get_dockq(line))\n",
    "\n",
    "    return info\n",
    "\n",
    "def capri_class(score):\n",
    "    if score >= 0.80:\n",
    "        return 'High'\n",
    "    elif 0.80 > score >= 0.49:\n",
    "        return 'Medium'\n",
    "    elif 0.49 > score >= 0.23:\n",
    "        return 'Acceptable'\n",
    "    else:\n",
    "        return 'Incorrect'\n",
    "\n",
    "def get_info(fp):\n",
    "\n",
    "    all_info = []\n",
    "\n",
    "    for file in os.listdir(fp):\n",
    "        if file.endswith('.txt'):\n",
    "            file_path = os.path.join(fp, file)\n",
    "            info = file_info(file_path)\n",
    "            all_info.append(info)\n",
    "\n",
    "    df = pd.DataFrame(all_info)\n",
    "\n",
    "    # Assign capri scores\n",
    "    df['capri'] = df['dockq'].apply(capri_class)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = './project_pipeline/data/output/dockq/'\n",
    "ai_fd_cl = 'ai_full_depth_cluster'\n",
    "ai_pdb_cl = 'ai_pdb_cluster'\n",
    "ai_pdb_fd = 'ai_pdb_full_depth'\n",
    "md_fd_cl = 'md_full_depth_cluster'\n",
    "md_pdb_cl = 'md_pdb_cluster'\n",
    "md_pdb_fd = 'md_pdb_full_depth'\n",
    "\n",
    "fp_list = [ai_fd_cl, ai_pdb_cl, ai_pdb_fd, md_fd_cl, md_pdb_cl, md_pdb_fd]\n",
    "\n",
    "# for fp in fp_list:\n",
    "#     df = get_info(os.path.join(results_path, fp))\n",
    "#     df.to_csv(f'./project_pipeline/data/{fp}_dockq.csv', index=False)\n",
    "\n",
    "df = get_info(os.path.join(results_path, ai_pdb_cl))\n",
    "df.to_csv(f'./project_pipeline/data/{ai_pdb_cl}_dockq.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
