{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to retrieve multi-domain proteins for analysis of domain to domain predicted aligned error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we take the list of PDB IDs with permanent domain-domain interactions as determined by [Sidhanta _et al._, 2023](https://doi.org/10.1002/prot.26581) and use that to build our list of UniProt IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build the PDB Search API request\n",
    "'''\n",
    "\n",
    "# Set the search parameters\n",
    "def get_uniprot_ids_for_pdbs(pdb_ids):\n",
    "    uniprot_ids = []\n",
    "    for pdb in pdb_ids:\n",
    "        url = f\"https://data.rcsb.org/rest/v1/core/uniprot/{pdb}/1\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            # Parse the response to extract UniProt IDs\n",
    "            if 'rcsb_uniprot_container_identifiers' in data[0]:\n",
    "                if 'uniprot_id' in data[0]['rcsb_uniprot_container_identifiers']:\n",
    "                    uniprot_id = data[0]['rcsb_uniprot_container_identifiers']['uniprot_id']\n",
    "                    uniprot_ids.append(uniprot_id)\n",
    "        \n",
    "        else:\n",
    "            print(f\"Failed to fetch data: {response.status_code}\")\n",
    "\n",
    "    return uniprot_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1A62', '1H2W', '1NTY', '1S6Y', '1Y02', '2J3X', '3E2Q', '3TXN', '5N4F', '1A8D', '1IG8', '1OHC', '1SAT', '1YDX', '2J8G', '3F1W', '4A2B', '5T5X', '1AOA', '1IH7', '1P4X', '1SQW', '1Z5V', '2NLK', '3FUQ', '4EQ3', '5UV2', '1CDY', '1IHG', '1PI6', '1T1E', '1ZAR', '2VAP', '3GON', '4FJQ', '5VF5', '1DHY', '1J5Y', '1Q7H', '1TXD', '1ZRL', '2VXR', '3H7I', '4FQZ', '6APE', '1EIF', '1K1S', '1Q8I', '1U3D', '2B3X', '2WMF', '3LNU', '4GQ4', '1EWF', '1KHB', '1QS2', '1U5P', '2D5B', '2WN4', '3MZF', '4HD9', '1F2Q', '1KJW', '1QTS', '1UCT', '2EF0', '2YHS', '3N7L', '4J12', '1F5N', '1L8Q', '1QZZ', '1UEK', '2FY2', '2YV1', '3OF1', '4LGN', '1F97', '1MD8', '1R2J', '1V0W', '2GNO', '2YZQ', '3SBS', '4O95', '1FVI', '1N4K', '1RHS', '1VCT', '2GNX', '3A0F', '3SQZ', '4XEH', '1G4R', '1NE9', '1RLR', '1W9H', '2I0K', '3BZ6', '3TEW', '5IL3', '1GV2', '1NR0', '1S5J', '1X6O', '2ILL', '3CE0', '3TRE', '5J4O']\n",
      "['P0AG30', 'P23687', 'O75962', 'P84135', 'Q8WZ73', 'O69275', 'P09546', 'Q7KLV9', 'H2E7Q8', 'P04958', 'P04807', 'O60729', 'P23694', 'Q49434', 'P15057', 'P15873', 'Q9WZU0', 'P97784', 'P13797', 'Q38087', 'Q2G1N7', 'Q9Y221', 'P23258', 'P23470', 'A7GBG3', 'B4XN22', 'A0A1C9J6A7', 'P01730', 'P26882', 'P46680', 'Q8RR56', 'O30245', 'Q57816', 'Q8DR49', 'M4GGS0', 'A0A2R2JFS1', 'P17297', 'Q9X1T8', 'Q9HIB8', 'Q9NZN5', 'Q25735', 'Q60393', 'P13319', 'O00214', 'B5Z6U8', 'Q58625', 'P96022', 'P21189', 'Q43125', 'P21399', 'A0A0H2US34', 'Q5GYJ8', 'O00255', 'P17213', 'P35558', 'Q844J9', 'P07751', 'P23395', 'Q9KH42', 'P0AEB2', 'Q13477', 'P12319', 'P31016', 'P17427', 'P24071', 'Q5SJ15', 'P10121', 'Q9LBR1', 'P01857', 'P32455', 'O66659', 'Q54527', 'P83700', 'P28329', 'Q58643', 'P07278', 'A0LSI1', 'O88792', 'P00736', 'Q9KIE5', 'P84147', 'Q9WZM9', 'O59416', 'P32357', 'E3T1W8', 'A7RCB1', 'P11881', 'P00586', 'O57975', 'Q6P1I3', 'Q764N8', 'Q8DUI5', 'E0SRA9', 'P17870', 'Q9EY50', 'P00452', 'O28951', 'Q7SID9', 'Q882E2', 'P13423', 'Q40577', 'P06876', 'Q11176', 'P26811', 'A4HE05', 'Q8WZ42', 'Q9Y6F1', 'Q83AR4', 'P02549']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "\n",
    "pdb_ids = []\n",
    "\n",
    "with open ('./project_pipeline/data/sidhanta_pdbs.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        pdb_ids.append(''.join(row).upper())\n",
    "\n",
    "print(pdb_ids)\n",
    "uniprot_ids = get_uniprot_ids_for_pdbs(pdb_ids)\n",
    "print(uniprot_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./project_pipeline/data/sidhanta_uniprots.csv', 'w') as f:\n",
    "    for uniprot in uniprot_ids:\n",
    "        f.write(uniprot + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the domain information for each UniProt ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Commented out\n",
    "'''\n",
    "def get_domains(uniprot_id):\n",
    "    '''\n",
    "    Get the domain information from UniProtKB\n",
    "    '''\n",
    "    print(f'Getting domains for {uniprot_id}')\n",
    "    url = f'https://rest.uniprot.org/uniprotkb/search?query=accession:{uniprot_id}&fields=ft_domain'\n",
    "    response = requests.get(url)\n",
    "    response_dic = response.json()\n",
    "    domains = []\n",
    "    try:\n",
    "        features = response_dic['results'][0]['features']\n",
    "        # Get the start and end of any domains\n",
    "        for i in range(len(features)):\n",
    "            if response_dic['results'][0]['features'][0]['type'] == 'Domain':\n",
    "                start = str(features[i]['location']['start']['value'])\n",
    "                end = str(features[i]['location']['end']['value'])\n",
    "                domains.append((start + '-' + end))\n",
    "\n",
    "        domains_string = ','.join(domains)\n",
    "\n",
    "    except KeyError:\n",
    "        print(f'No domains found for {uniprot_id}')\n",
    "        domains_string = None\n",
    "\n",
    "    return domains_string\n",
    "\n",
    "# def single_domains(uniprots):\n",
    "#     '''\n",
    "#     Get the single domains from UniProtKB\n",
    "#     '''\n",
    "#     domains = {'uniprot': [], 'region': []}\n",
    "#     # Get domains for the uniprot ids\n",
    "#     for i in range(len(uniprots)):\n",
    "#         uniprot_id = uniprots[i]\n",
    "#         region = get_domains(uniprot_id)\n",
    "#         domains['uniprot'].append(uniprot_id)\n",
    "#         domains['region'].append(region)\n",
    "\n",
    "#     # Convert to pandas dataframe\n",
    "#     domains_df = pd.DataFrame.from_dict(domains, orient='columns')\n",
    "\n",
    "#     return domains_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting domains for P0AG30\n",
      "Getting domains for P23687\n",
      "Getting domains for O75962\n",
      "Getting domains for P84135\n",
      "Getting domains for Q8WZ73\n",
      "Getting domains for O69275\n",
      "Getting domains for P09546\n",
      "Getting domains for Q7KLV9\n",
      "Getting domains for H2E7Q8\n",
      "Getting domains for P04958\n",
      "Getting domains for P04807\n",
      "Getting domains for O60729\n",
      "Getting domains for P23694\n",
      "Getting domains for Q49434\n",
      "Getting domains for P15057\n",
      "Getting domains for P15873\n",
      "Getting domains for Q9WZU0\n",
      "Getting domains for P97784\n",
      "Getting domains for P13797\n",
      "Getting domains for Q38087\n",
      "Getting domains for Q2G1N7\n",
      "Getting domains for Q9Y221\n",
      "Getting domains for P23258\n",
      "Getting domains for P23470\n",
      "Getting domains for A7GBG3\n",
      "Getting domains for B4XN22\n",
      "Getting domains for A0A1C9J6A7\n",
      "Getting domains for P01730\n",
      "Getting domains for P26882\n",
      "Getting domains for P46680\n",
      "Getting domains for Q8RR56\n",
      "Getting domains for O30245\n",
      "Getting domains for Q57816\n",
      "Getting domains for Q8DR49\n",
      "Getting domains for M4GGS0\n",
      "Getting domains for A0A2R2JFS1\n",
      "Getting domains for P17297\n",
      "Getting domains for Q9X1T8\n",
      "Getting domains for Q9HIB8\n",
      "Getting domains for Q9NZN5\n",
      "Getting domains for Q25735\n",
      "Getting domains for Q60393\n",
      "Getting domains for P13319\n",
      "Getting domains for O00214\n",
      "Getting domains for B5Z6U8\n",
      "Getting domains for Q58625\n",
      "Getting domains for P96022\n",
      "Getting domains for P21189\n",
      "Getting domains for Q43125\n",
      "Getting domains for P21399\n",
      "Getting domains for A0A0H2US34\n",
      "Getting domains for Q5GYJ8\n",
      "Getting domains for O00255\n",
      "Getting domains for P17213\n",
      "Getting domains for P35558\n",
      "Getting domains for Q844J9\n",
      "Getting domains for P07751\n",
      "Getting domains for P23395\n",
      "Getting domains for Q9KH42\n",
      "Getting domains for P0AEB2\n",
      "Getting domains for Q13477\n",
      "Getting domains for P12319\n",
      "Getting domains for P31016\n",
      "Getting domains for P17427\n",
      "Getting domains for P24071\n",
      "Getting domains for Q5SJ15\n",
      "Getting domains for P10121\n",
      "Getting domains for Q9LBR1\n",
      "Getting domains for P01857\n",
      "Getting domains for P32455\n",
      "Getting domains for O66659\n",
      "Getting domains for Q54527\n",
      "Getting domains for P83700\n",
      "Getting domains for P28329\n",
      "Getting domains for Q58643\n",
      "Getting domains for P07278\n",
      "Getting domains for A0LSI1\n",
      "Getting domains for O88792\n",
      "Getting domains for P00736\n",
      "Getting domains for Q9KIE5\n",
      "Getting domains for P84147\n",
      "Getting domains for Q9WZM9\n",
      "Getting domains for O59416\n",
      "Getting domains for P32357\n",
      "Getting domains for E3T1W8\n",
      "Getting domains for A7RCB1\n",
      "Getting domains for P11881\n",
      "Getting domains for P00586\n",
      "Getting domains for O57975\n",
      "Getting domains for Q6P1I3\n",
      "Getting domains for Q764N8\n",
      "Getting domains for Q8DUI5\n",
      "Getting domains for E0SRA9\n",
      "Getting domains for P17870\n",
      "Getting domains for Q9EY50\n",
      "Getting domains for P00452\n",
      "Getting domains for O28951\n",
      "Getting domains for Q7SID9\n",
      "Getting domains for Q882E2\n",
      "Getting domains for P13423\n",
      "Getting domains for Q40577\n",
      "Getting domains for P06876\n",
      "Getting domains for Q11176\n",
      "Getting domains for P26811\n",
      "Getting domains for A4HE05\n",
      "Getting domains for Q8WZ42\n",
      "Getting domains for Q9Y6F1\n",
      "Getting domains for Q83AR4\n",
      "Getting domains for P02549\n"
     ]
    }
   ],
   "source": [
    "domain_dict = {'uniprot': [], 'domains': []}\n",
    "\n",
    "for uniprot in uniprot_ids:\n",
    "    domains = get_domains(uniprot)\n",
    "    domain_dict['uniprot'].append(uniprot)\n",
    "    domain_dict['domains'].append(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot</th>\n",
       "      <th>domains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0AG30</td>\n",
       "      <td>48-123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P23687</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O75962</td>\n",
       "      <td>65-210,1292-1467,1480-1591,1656-1721,1969-2145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P84135</td>\n",
       "      <td>200-416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q8WZ73</td>\n",
       "      <td>101-120,250-264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniprot                                            domains\n",
       "0  P0AG30                                             48-123\n",
       "1  P23687                                                   \n",
       "2  O75962  65-210,1292-1467,1480-1591,1656-1721,1969-2145...\n",
       "3  P84135                                            200-416\n",
       "4  Q8WZ73                                    101-120,250-264"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to dataframe\n",
    "domain_df = pd.DataFrame.from_dict(domain_dict, orient='columns')\n",
    "domain_df.head()\n",
    "\n",
    "# Save to csv\n",
    "# domain_df.to_csv('./project_pipeline/data/sidhanta_domains.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot</th>\n",
       "      <th>domains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0AG30</td>\n",
       "      <td>48-123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P23687</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O75962</td>\n",
       "      <td>65-210,1292-1467,1480-1591,1656-1721,1969-2145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P84135</td>\n",
       "      <td>200-416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q8WZ73</td>\n",
       "      <td>101-120,250-264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniprot                                            domains\n",
       "0  P0AG30                                             48-123\n",
       "1  P23687                                                   \n",
       "2  O75962  65-210,1292-1467,1480-1591,1656-1721,1969-2145...\n",
       "3  P84135                                            200-416\n",
       "4  Q8WZ73                                    101-120,250-264"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove any proteins that may overlap with my autoinhibited set\n",
    "autoinhibited = pd.read_csv('./project_pipeline/data/classified_files_3.tsv', sep='\\t').astype('object')\n",
    "common = domain_df['uniprot'].isin(autoinhibited['uniprot'])\n",
    "domains_df = domain_df.drop(domain_df[common].index).reset_index(drop=True)\n",
    "\n",
    "# Save the dataframe\n",
    "domains_df.to_csv('./project_pipeline/data/sidhanta_domains.csv', index=False)\n",
    "domains_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we take the list of proteins defined as \"multi-domain\" by SCOPe on the PDB and filter those down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build the PDB Search API request\n",
    "'''\n",
    "\n",
    "# Set the search parameters\n",
    "url = 'https://search.rcsb.org/rcsbsearch/v2/query'\n",
    "query = {\n",
    "  \"query\": {\n",
    "    \"type\": \"group\",\n",
    "    \"nodes\": [\n",
    "      {\n",
    "        \"type\": \"group\",\n",
    "        \"logical_operator\": \"and\",\n",
    "        \"nodes\": [\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_polymer_instance_annotation.annotation_lineage.id\",\n",
    "              \"operator\": \"exact_match\",\n",
    "              \"value\": \"56572\",\n",
    "              \"negation\": False\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_polymer_instance_annotation.type\",\n",
    "              \"operator\": \"exact_match\",\n",
    "              \"value\": \"SCOP\",\n",
    "              \"negation\": False\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"label\": \"nested-attribute\"\n",
    "      },\n",
    "    ],\n",
    "    \"logical_operator\": \"and\",\n",
    "    \"label\": \"text\"\n",
    "  },\n",
    "  \"return_type\": \"polymer_entity\",\n",
    "  \"request_options\": {\n",
    "    \"group_by_return_type\": \"groups\",\n",
    "    \"group_by\": {\n",
    "      \"aggregation_method\": \"matching_uniprot_accession\",\n",
    "      \"ranking_criteria_type\": {\n",
    "        \"sort_by\": \"rcsb_entry_info.resolution_combined\",\n",
    "        \"direction\": \"asc\"\n",
    "      }\n",
    "    },\n",
    "    \"return_all_hits\": True,\n",
    "    \"results_content_type\": [\n",
    "      \"experimental\"\n",
    "    ],\n",
    "    \"sort\": [\n",
    "      {\n",
    "        \"sort_by\": \"score\",\n",
    "        \"direction\": \"desc\"\n",
    "      },\n",
    "      {\n",
    "        \"sort_by\": \"size\",\n",
    "        \"direction\": \"desc\"\n",
    "      }\n",
    "    ],\n",
    "    \"scoring_strategy\": \"combined\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url, json=query)\n",
    "\n",
    "response_dic = response.json()\n",
    "\n",
    "# Get the PDB IDs\n",
    "group_set = response_dic['group_set']\n",
    "\n",
    "# Get the list of uniprot ids\n",
    "uniprots = [group_set[i]['identifier'] for i in range(len(group_set))]\n",
    "\n",
    "\n",
    "with open('./project_pipeline/data/multi_domain_uniprots.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['uniprot'])\n",
    "    writer.writerows(uniprots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Commented out\n",
    "'''\n",
    "# domains_df = domains_df.drop(domains_df[domains_df['region'] == ''].index).reset_index(drop=True)\n",
    "# domains_df = domains_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# # Remove any proteins with only one annotated domain\n",
    "# for i in range(len(domains_df)):\n",
    "#     region = domains_df.loc[i, 'region']\n",
    "#     count = region.count('-')\n",
    "#     if count <= 1:\n",
    "#         domains_df = domains_df.drop(i)\n",
    "\n",
    "# # remove any proteins that may overlap with my autoinhibited set\n",
    "# autoinhibited = pd.read_csv('./project_pipeline/data/classified_files_3.tsv', sep='\\t').astype('object')\n",
    "# common = domains_df['uniprot'].isin(autoinhibited['uniprot'])\n",
    "# print(common)\n",
    "# domains_df = domains_df.drop(domains_df[common].index).reset_index(drop=True)\n",
    "\n",
    "# # Save the dataframe\n",
    "# domains_df.to_csv('./project_pipeline/data/multi_domain_domains.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoinhibited = pd.read_csv('./project_pipeline/data/classified_files_3.tsv', sep='\\t').astype('object')\n",
    "single_df = pd.read_csv('./project_pipeline/data/single_domain_domains.csv')\n",
    "common2 = single_df['uniprot'].isin(autoinhibited['uniprot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "How many multi-domain proteins have only two designated domains?\n",
    "'''\n",
    "md = pd.read_csv('./project_pipeline/data/multi_domain_domains.csv')\n",
    "\n",
    "# Get the number of domains\n",
    "md['num_domains'] = md['region'].str.count(',') + 1\n",
    "\n",
    "# Get the number of proteins with only two domains\n",
    "md_2 = md[md['num_domains'] == 2]\n",
    "print(len(md_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Select proteins with two domains. Label those domains 'region 1' and 'region 2' and pass the protein\n",
    "through my predicted aligned error script.\n",
    "'''\n",
    "domains_df = pd.read_csv('./project_pipeline/data/multi_domain_domains.csv').astype('object')\n",
    "\n",
    "domains_df['region'] = domains_df['region'].str.split(',')\n",
    "two_domains_list = [row for index, row in domains_df.iterrows() if len(row['region']) == 2]\n",
    "two_domains = pd.DataFrame(two_domains_list).reset_index(drop=True)\n",
    "\n",
    "expand = pd.DataFrame(two_domains['region'].to_list(), columns=['region_1', 'region_2'])\n",
    "expand = expand.join(two_domains['uniprot'], how='left')\n",
    "expand = expand[['uniprot', 'region_1', 'region_2']]\n",
    "expand.head()\n",
    "\n",
    "expand.to_csv('./project_pipeline/data/multi_domain_regions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step assumes that scripts/control_domains_pae.py has been run. We make a file to be run in our snakemake multi-domain pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out proteins in multi_domain_regions.tsv that are not in multi_domain_pae.tsv\n",
    "multi_domain_regions = pd.read_csv('./project_pipeline/data/multi_domain_regions.tsv', sep='\\t')\n",
    "multi_domain_pae = pd.read_csv('./project_pipeline/data/multi_domain_pae.tsv', sep='\\t')\n",
    "\n",
    "common = multi_domain_regions['uniprot'].isin(multi_domain_pae['uniprot'])\n",
    "multi_domain_regions = multi_domain_regions[common]\n",
    "multi_domain_regions.to_csv('./project_pipeline/data/multi_domain.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rmsd_snek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
